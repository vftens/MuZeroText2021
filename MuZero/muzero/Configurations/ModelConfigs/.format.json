{
  "name": "format",
  "algorithm": "Algorithm name (Currently only 'MUZERO' or 'ALPHAZERO' implemented)",
  "architecture": "What neural architecture to use. For the currently implemented ones @see Agents/__init__.py",

  "args": {
    "num_selfplay_iterations": "(int) Number of iterations to repeat the training loop (self play - training - pitting)",
    "num_episodes": "(int) Number of episodes to perform self play for collecting training examples",
    "num_gradient_steps": "(int) Number of weight updates to perform in the backpropagation step",
    "max_episode_moves": "(int) Number of steps until termination during self play.",
    "max_trial_moves": "(int) Number of steps until termination during pitting/ testing.",
    "pitting": "(bool) Whether to test the newly trained model against the old model in a trial",
    "pitting_trials": "(int > 1) Number of trials to test the new model against the old",
    "pit_acceptance_ratio": "(double: [0, 1]) Win/loss ratio for whether to accept the new model over the old model ",
    "dirichlet_alpha": "(double) Alpha parameter of the dirichlet distribution to sample noise from for exploration",
    "exploration_fraction": "(double: [0, 1]) Fraction to sample based on noise of the dirichlet for exploration vs the network prior",
    "max_buffer_size": "(int) Maximum number of game examples to train the neural networks on",
    "num_MCTS_sims": "(int) Number of planning moves for MCTS to simulate",
    "prioritize": "(bool) Set to true when using prioritized sampling from the replay buffer (used in Atari)",
    "prioritize_alpha": "(double) Exponentiation factor for computing probabilities in prioritized replay",
    "prioritize_beta": "(double) Exponentiation factor for exponentiating the importance sampling ratio in prioritized replay",
    "latent_decoder": "(bool) MuZero option argument to jointly train a latent-state to real-state decoder model",
    "K": "(int > 0) Number of 'hypothetical' steps to unroll the MuZero network for during backpropagation",
    "n_steps": "(int > 0) Amount of steps to look ahead for rewards before bootstrapping (value function estimation)",
    "c1": "(double) First exploration constant for MuZero in the PUCT formula",
    "c2": "(double) Second exploration constant for MuZero in the PUCT formula",
    "gamma": "(double: [0, 1]) MDP Discounting factor for future rewards",

    "minimum_reward": "(double) Lower bound on the cumulative reward of the environment (null if not known)",
    "maximum_reward": "(double) Upper bound on the cumulative reward of the environment (null if not known)",

    "checkpoint": "(path: string) Directory to store sampled experiences and fitted parameters in after every selfplay loop",
    "load_model": "(bool) Whether to load in a previously trained model from the checkpoint directory when starting training.",
    "load_folder_file": "(Array, ['directory', 'file']) Specify which model to load in/ store the best current model in.",
    "selfplay_buffer_window": "(int) Maximum number of self play iterations kept in the deque.",

    "temperature_schedule": {
      "method": "(string) choice between 'stepwise' and 'linear'.",
      "by_weight_update": "(bool) Whether to adjust the temperature according to weight updates OR episode steps",
      "schedule_points": "[[step, value], [step, value]]: Array to govern the temperature schedule"
    }
  },

  "net_args": {
    "optimizer": {
      "method": "(string) Optimizer to use for model. MuZero has support for 'sgd' and 'adam'",
      "lr_init": "(double) Learning rate for the neural network's optimizer (initial learning rate if schedule is used)",
      "momentum": "(double) Momentum used for optimizer when using SGD."
    },
    "l2": "(double) Penalty scalar for l2 loss of network weights",
    "dynamics_penalty": "(double) Penalty for MuZero dynamics model for diverging too far from the representation network/ true transition function",
    "dropout": "(double) DropOut regularization rate for the neural network",
    "batch_size": "(int) Stochastic gradient descent batch size",
    "num_channels": "(int) Number of feature maps in the convolutional network",
    "num_towers": "(int) Number of convolutional towers to place in each network",
    "num_dense": "(int) Number of fully connected layers to use as the head of the CNN",
    "size_dense": "(int) Number of hidden nodes to use in the fully connected layers",
    "support_size": "(int) Number of integers for the reward distribution (defaults to a Gaussian if set to 0)",
    "observation_length": "(int) Number of observations in the history to be passed into the encoder"
  }
}
