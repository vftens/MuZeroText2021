{
  "name": "(str) Name of your experiment.",
  "experiment": "(str) Choice of experiment @see Experimenter/__init__.py for all available implementations",
  "output_dir": "(str) Name/ path of an output directory",

  "checkpoint_resolution": "(int > 0) Number indicating: evaluate each 'x' models from my 'N' model checkpoints for an evaluation tournament",
  "num_repeat": "(int > 0) Repetition factor for repeating an experiment",
  "num_trials": "(int > 0) Number of evaluation runs to perform when testing a model/ performing an evaluation tournament",
  "num_opponents": "(int > 0 or null) Number of games to play per player in adversarial games, null is exhaustive",
  "n_jobs": "(int > 0) Number of threads to use if experiment runs asynchronously",
  "flags": "(str) console flags like '--debug' or '--render' to add when running ablations",

  "environment": {
    "name": "(str) Choice of environment to test on @see Games/__init__.py for all available implementations",
    "console": "(str) Console argument to this environment, e.g., '--game hex' or '--game atari_breakout'",
    "args": "(dict) Environment specific arguments"
  },

  "players": [
    {
      "name": "(str) Choice of agent to test @see Agents/__init__.py for all available implementations",
      "config": "(str) Path to .json configuration file for the given agent"
    }
  ],

  "ablations": {
    "base": {
      "name": "(str) Choice of the model to train over all ablations",
      "config": "(str) Path to .json configuration file for the given agent"
    },
    "content": {
      "(param_name)": "(list of values to override in the base json), an example is: 'example_key': ['value1', 'value2']"
    }
  }
}
